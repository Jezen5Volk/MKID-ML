This directory contains the code that I wrote for the statistical implementation of my machine learning model. After my first attempt at implementation (see "First Implementation" directory, inside the "Naive Loss Function" directory), I sought to improve performance. A classic tenet of machine learning (and data science in general) is garbage in, garbage out. If I wanted to improve performance, I would need to start by improving the way I generated synthetic data. 

I had a dataset containing fifty thousand photon incidence events across five different wavelengths of light. The way I had been previously stitching them together to get a synthetic dataset involved generating a dataset of noise, then inserting photons on top of it. This resulted in discontinuities in the data set that aren't present in an actual dataset. To mitigate this, I generated a mapping between each of the fifty thousand photon incidence events. The ith element of the mapping contained an index for the photon incidence event which had the least discontinuous data. This mapping was somewhat computationally expensive to generate, so it was precomputed and then called repeatedly during the synthetic data generation step.

The next step was to improve the actual machine learning model itself. In order to be able to identify the energy of incident photons and the time at which they are incident in each incidence event, I utilized a loss function which had five terms. Each term penalized different aspects of prediction: variance in time, variance in energy, incorrect number of photons predicted, incorrect energy predictions, and incorrect time predictions. Implementing this loss function in tensorflow was incredibly difficult and resulted in changing API twice. After four months of programming, and several false starts, I was able to get the model to train without erroring.

In tragic form, the loss function was ultimately unsuccessful in penalizing the predictions, and outputs were essentially gibberish.
