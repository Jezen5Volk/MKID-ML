At this point in the project, I was spending a lot of time learning about tensorflow and how exactly to "do" machine learning. I got "results" but hadn't written much code to evaluate my model once it had been trained. I was rather uncertain about how to improve model performance, and changing hyperparameters has mixed results.

Additionally, the way synthetic data was being generated was that I loaded a dictionary with 5 different wavelengths worth of photon incidence events (808 nm, 920 nm, 980 nm, 1120 nm, and 1310 nm), each of which had ten thousand photon incidences. Each photon incidence event consisted of three numpy arrays: I trace (the real axis of phase space), Q trace (the imaginary axis of phase space), and the Filtered Response (what the model hoped to reproduce). First, the noise at the beginning of each data trace from the photon incidence events was stitched together to create an array equivalent to a preset length of synthetic data being passed. Based on the length of the synthetic data being generated and on the count rate of photons, a number of photon incidences was generated. These were than randomly inserted into the generated synthetic data, overwriting the noise at those indices. 

The downside to this is that this procedure introduced a number of discontinuities, and the machine learing model was learning the pattern of the jumps for the discontinuities much better than it was learning the pattern for the signal we wanted to capture.
