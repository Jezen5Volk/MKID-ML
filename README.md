# MKID-ML
Over the course of this project, I took several approaches towards attempting to use Tensorflow to replicate the effects of FIR filtering/"Optimal" Filtration, which is the current implementation for analyzing data from the MKID arrays. Optimal filtration provides a series of weights used to process data in a way that is proven to be mathematically optimal. However, the data from the MKID arrays violates some of the fundamental assumptions of Optimal Filtering, thus Machine Learning provides an opportunity to possibly get better performance and replace a computationally expensive step.

The first approach was the "most naive" possible approach: I generated synthetic data in the most naive possible way and I wrote the Machine Learning model in the most naive way possible. I also had very little in the way of code to evaluate and characterize the models I generated. 

Since this didn't yield very good results, I eventually ended up attempting to focus on a statistical implementation of the machine learning model. I also added a good deal of intelligent checking when generating the synthetic dataset to make it more representative of the actual data the model would see. The statistical model was very difficult to implement in Tensorflow, and took me about six months of work (interrupted by the summer, so about 9 months of time) just to write the custom subclassed Tensorflow API in order to get the model to train without erroring out. Much to my dismay, the results it produced were inaccurate and imprecise.

After speaking with my advisor, I was encouraged to go back to the "naive" model (I go into more depth in the documentation, but essentially a least squares fit) with my now upgraded synthetic data generation. Additionally, at this point I had a number of characterization tools which I had developed while coding the statistical implementation that I could use to circumvent model inaccuracy under the premise of model precision, so those six months weren't a total loss. At this point, I'd also become a much more competent programmer. After a few weeks of work, I generated a model which successfully captured the behavior of a photon incidence event. 

A little characterization and much evaluation code later, I had results! Though my model worked, it was a little bit of a black box. We weren't sure why exactly it worked (other than "Tensorflow magic go brrr") or where it might break down. To explore this, I was asked to interface my machine learning model with a new method of generating synthetic data: a python package which modeled the quasiparticle density of the MKID with poisson distributed photon incidence events, written by one of the grad students in the group. I modified her code and used my Tensorflow code to train a model on the new "quasiparticle synthetic data". I then characterized and evaluated this model. Then, I handed off the project to a graduate student, who is planning on exploring an alternate method of generating the machine learning model. My advisor told me that the work I have done is publishable and that I will likely be second author on any publications that come from the combination of my work and the work of the graduate student who the project has been handed off to. 

Picture Gallery: 

<img width="400" alt="Predicted Photon" src="https://github.com/Jezen5Volk/MKID-ML/assets/138075884/e1af8d34-6998-4fad-8cdc-d70091ca33fe">
<img width="800" alt="histogram of predicted photons with fit" src="https://github.com/Jezen5Volk/MKID-ML/assets/138075884/3bd429e3-e63d-43a8-a514-de0271c3dff3">
<img width="800" alt="quasiparticle predicted pdfs" src="https://github.com/Jezen5Volk/MKID-ML/assets/138075884/17ec2ce2-5560-4ea7-98df-543f603eaf52">

